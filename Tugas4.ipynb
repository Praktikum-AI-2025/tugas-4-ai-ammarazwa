{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327e03f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder C:\\Users\\maraa\\tensorflow_datasets\\horses_or_humans\\3.0.0 has no dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\maraa\\tensorflow_datasets\\horses_or_humans\\3.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maraa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Size...: 100%|██████████| 152/152 [11:20<00:00,  4.48s/ MiB]l]\n",
      "Dl Completed...: 100%|██████████| 2/2 [11:20<00:00, 340.14s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset horses_or_humans downloaded and prepared to C:\\Users\\maraa\\tensorflow_datasets\\horses_or_humans\\3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maraa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 585ms/step - accuracy: 0.5985 - loss: 0.9616 - val_accuracy: 0.8835 - val_loss: 0.2651\n",
      "Epoch 2/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 478ms/step - accuracy: 0.9328 - loss: 0.1777 - val_accuracy: 0.9854 - val_loss: 0.0587\n",
      "Epoch 3/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 476ms/step - accuracy: 0.9712 - loss: 0.0704 - val_accuracy: 0.9806 - val_loss: 0.0585\n",
      "Epoch 4/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 480ms/step - accuracy: 0.9929 - loss: 0.0269 - val_accuracy: 1.0000 - val_loss: 0.0047\n",
      "Epoch 5/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 478ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 1.0000 - val_loss: 9.2651e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 504ms/step - accuracy: 1.0000 - loss: 5.7938e-04 - val_accuracy: 1.0000 - val_loss: 8.9413e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 495ms/step - accuracy: 1.0000 - loss: 1.5243e-04 - val_accuracy: 1.0000 - val_loss: 3.3305e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 463ms/step - accuracy: 1.0000 - loss: 1.7089e-04 - val_accuracy: 1.0000 - val_loss: 4.3759e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 456ms/step - accuracy: 1.0000 - loss: 1.1095e-04 - val_accuracy: 1.0000 - val_loss: 2.4699e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 525ms/step - accuracy: 1.0000 - loss: 5.8644e-05 - val_accuracy: 1.0000 - val_loss: 2.0010e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 506ms/step - accuracy: 1.0000 - loss: 4.5529e-05 - val_accuracy: 1.0000 - val_loss: 1.6083e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 464ms/step - accuracy: 1.0000 - loss: 4.8409e-05 - val_accuracy: 1.0000 - val_loss: 1.5638e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 506ms/step - accuracy: 1.0000 - loss: 3.9903e-05 - val_accuracy: 1.0000 - val_loss: 1.3880e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 471ms/step - accuracy: 1.0000 - loss: 2.5006e-05 - val_accuracy: 1.0000 - val_loss: 1.1252e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 495ms/step - accuracy: 1.0000 - loss: 3.0792e-05 - val_accuracy: 1.0000 - val_loss: 1.1039e-04\n",
      "\n",
      "🎯 Final Train Accuracy: 100.00%\n",
      "🧪 Final Validation Accuracy: 100.00%\n",
      "✅ Model memenuhi standar akurasi (> 83%)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 📚 Import libraries\n",
    "# ==========================\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# ==========================\n",
    "# 📦 Load and preprocess dataset\n",
    "# ==========================\n",
    "# Load dataset dari TFDS\n",
    "dataset, info = tfds.load('horses_or_humans', with_info=True, as_supervised=True)\n",
    "train_dataset_full = dataset['train']\n",
    "\n",
    "# Split jadi 80% train dan 20% validation\n",
    "train_size = int(0.8 * info.splits['train'].num_examples)\n",
    "val_size = info.splits['train'].num_examples - train_size\n",
    "\n",
    "train_dataset = train_dataset_full.take(train_size)\n",
    "val_dataset = train_dataset_full.skip(train_size)\n",
    "\n",
    "# Resize dan normalize gambar\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (150, 150))\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).shuffle(1000).batch(32)\n",
    "val_dataset = val_dataset.map(preprocess).batch(32)\n",
    "\n",
    "# ==========================\n",
    "# 🧠 Define the model\n",
    "# ==========================\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# ==========================\n",
    "# ⚙️ Compile the model\n",
    "# ==========================\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ==========================\n",
    "# 🚀 Train the model\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 📊 Print Final Accuracy\n",
    "# ==========================\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"\\nFinal Train Accuracy: {final_train_acc * 100:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc * 100:.2f}%\")\n",
    "\n",
    "if final_train_acc >= 0.83 and final_val_acc >= 0.83:\n",
    "    print(\"Model memenuhi standar akurasi (> 83%)\")\n",
    "else:\n",
    "    print(\"Model belum memenuhi standar, perlu ditingkatkan lagi\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
